{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d42342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,math,pickle,random\n",
    "import numpy as np,pandas as pd\n",
    "import torch,torch._dynamo\n",
    "from model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ad3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1337\n",
    "out_dir = 'instructions'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "model_dir='long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f27090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fed177",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e09fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.37M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(55, 216, padding_idx=27)\n",
       "    (wpe): Embedding(50, 216)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=216, out_features=648, bias=False)\n",
       "          (c_proj): Linear(in_features=216, out_features=216, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=216, out_features=864, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=864, out_features=216, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=216, out_features=55, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(model_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "config=checkpoint[\"config\"]\n",
    "model = GPT(config)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open(os.path.join(out_dir, 'train.bin'),\"r\")\n",
    "train_names=[]\n",
    "evrythng=fp.readlines()\n",
    "for line in evrythng:\n",
    "    train_names.append(line.split(\"\\n\")[0])\n",
    "fp.close()\n",
    "fp=open(os.path.join(out_dir, 'val.bin'),\"r\")\n",
    "val_names=[]\n",
    "evrythng=fp.readlines()\n",
    "for line in evrythng:\n",
    "    val_names.append(line.split(\"\\n\")[0])\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad53266",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "block_size=config[\"block_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b086a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = config[\"n_layer\"]\n",
    "n_head = config[\"n_head\"]\n",
    "n_embd = config[\"n_embd\"]\n",
    "dropout = config[\"dropout\"]\n",
    "bias=config[\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5516da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "max_iters = 2000\n",
    "lr_decay_iters = 2000\n",
    "min_lr = 1e-4\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "warmup_iters = 50\n",
    "grad_clip = 1.0\n",
    "decay_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74df2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 30\n",
    "log_interval = 10\n",
    "eval_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e473602",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "dtype='float16'\n",
    "compile=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36887b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 1,600\n"
     ]
    }
   ],
   "source": [
    "tokens_per_iter =  batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401658cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11acc93d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_names if split == 'train' else val_names\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    pad_token=stoi[\"*\"]\n",
    "    x=torch.ones(batch_size,block_size,dtype=torch.long)*pad_token\n",
    "    y=torch.ones(batch_size,block_size,dtype=torch.long)*pad_token\n",
    "    for i,index in enumerate(ix):\n",
    "        sep=data[index].index(\"!\")\n",
    "        encoded=encode(data[index][:sep])+encode(data[index][sep+1:])\n",
    "        x[i][:len(encoded)-1]=torch.Tensor(encoded[:-1])\n",
    "        y[i][sep:len(encoded)-1]=torch.Tensor(encoded[1+sep:])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644f16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 3,381,912 parameters\n",
      "num non-decayed parameter tensors: 13, with 2,808 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44287bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8ac3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de00bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.6463, val loss 2.6230\n",
      "iter 0: loss 2.3666, time 2587.52ms\n",
      "iter 10: loss 2.0301, time 76.20ms\n",
      "iter 20: loss 2.1836, time 76.10ms\n",
      "step 30: train loss 1.9384, val loss 2.0225\n",
      "saving checkpoint to instructions\n",
      "iter 30: loss 2.0489, time 2359.26ms\n",
      "iter 40: loss 1.9623, time 76.08ms\n",
      "iter 50: loss 1.9078, time 75.89ms\n",
      "step 60: train loss 1.8705, val loss 1.9165\n",
      "saving checkpoint to instructions\n",
      "iter 60: loss 1.8445, time 2333.62ms\n",
      "iter 70: loss 2.0422, time 76.09ms\n",
      "iter 80: loss 1.9649, time 76.74ms\n",
      "step 90: train loss 1.8315, val loss 1.8661\n",
      "saving checkpoint to instructions\n",
      "iter 90: loss 1.9063, time 2382.39ms\n",
      "iter 100: loss 1.9285, time 75.65ms\n",
      "iter 110: loss 1.9380, time 75.73ms\n",
      "step 120: train loss 1.8079, val loss 1.8313\n",
      "saving checkpoint to instructions\n",
      "iter 120: loss 1.7963, time 2380.94ms\n",
      "iter 130: loss 1.7950, time 75.97ms\n",
      "iter 140: loss 1.8822, time 76.71ms\n",
      "step 150: train loss 1.7500, val loss 1.8126\n",
      "saving checkpoint to instructions\n",
      "iter 150: loss 1.8518, time 2340.01ms\n",
      "iter 160: loss 1.9618, time 75.91ms\n",
      "iter 170: loss 1.7654, time 75.85ms\n",
      "step 180: train loss 1.7296, val loss 1.7735\n",
      "saving checkpoint to instructions\n",
      "iter 180: loss 1.8409, time 2403.13ms\n",
      "iter 190: loss 1.8579, time 76.01ms\n",
      "iter 200: loss 1.7502, time 75.65ms\n",
      "step 210: train loss 1.7126, val loss 1.7829\n",
      "iter 210: loss 1.8448, time 2321.27ms\n",
      "iter 220: loss 1.7016, time 75.67ms\n",
      "iter 230: loss 1.6883, time 82.79ms\n",
      "step 240: train loss 1.7032, val loss 1.7818\n",
      "iter 240: loss 1.7872, time 2278.78ms\n",
      "iter 250: loss 1.7825, time 75.68ms\n",
      "iter 260: loss 1.8019, time 75.91ms\n",
      "step 270: train loss 1.6748, val loss 1.7807\n",
      "iter 270: loss 1.7140, time 2273.96ms\n",
      "iter 280: loss 1.7769, time 75.96ms\n",
      "iter 290: loss 1.7939, time 75.88ms\n",
      "step 300: train loss 1.6828, val loss 1.7506\n",
      "saving checkpoint to instructions\n",
      "iter 300: loss 1.6789, time 2335.20ms\n",
      "iter 310: loss 1.7922, time 75.51ms\n",
      "iter 320: loss 1.7012, time 75.99ms\n",
      "step 330: train loss 1.6540, val loss 1.7705\n",
      "iter 330: loss 1.6431, time 2286.79ms\n",
      "iter 340: loss 1.7140, time 76.18ms\n",
      "iter 350: loss 1.7041, time 75.91ms\n",
      "step 360: train loss 1.6739, val loss 1.7556\n",
      "iter 360: loss 1.6141, time 2304.37ms\n",
      "iter 370: loss 1.7296, time 79.67ms\n",
      "iter 380: loss 1.6986, time 79.06ms\n",
      "step 390: train loss 1.6630, val loss 1.7473\n",
      "saving checkpoint to instructions\n",
      "iter 390: loss 1.8833, time 2565.90ms\n",
      "iter 400: loss 1.7079, time 82.83ms\n",
      "iter 410: loss 1.7299, time 83.37ms\n",
      "step 420: train loss 1.6251, val loss 1.7504\n",
      "iter 420: loss 1.7701, time 2275.04ms\n",
      "iter 430: loss 1.8997, time 75.87ms\n",
      "iter 440: loss 1.6957, time 75.77ms\n",
      "step 450: train loss 1.6278, val loss 1.7433\n",
      "saving checkpoint to instructions\n",
      "iter 450: loss 1.7979, time 2332.86ms\n",
      "iter 460: loss 1.7626, time 75.82ms\n",
      "iter 470: loss 1.7063, time 75.82ms\n",
      "step 480: train loss 1.6502, val loss 1.7475\n",
      "iter 480: loss 1.8917, time 2280.41ms\n",
      "iter 490: loss 1.7701, time 76.72ms\n",
      "iter 500: loss 1.6712, time 76.15ms\n",
      "step 510: train loss 1.6281, val loss 1.7298\n",
      "saving checkpoint to instructions\n",
      "iter 510: loss 1.6811, time 2345.19ms\n",
      "iter 520: loss 1.7270, time 76.60ms\n",
      "iter 530: loss 1.5679, time 76.70ms\n",
      "step 540: train loss 1.6375, val loss 1.7255\n",
      "saving checkpoint to instructions\n",
      "iter 540: loss 1.7361, time 2354.46ms\n",
      "iter 550: loss 1.8471, time 81.54ms\n",
      "iter 560: loss 1.8597, time 75.91ms\n",
      "step 570: train loss 1.6023, val loss 1.7383\n",
      "iter 570: loss 1.7001, time 2290.74ms\n",
      "iter 580: loss 1.6207, time 75.98ms\n",
      "iter 590: loss 1.6575, time 77.00ms\n",
      "step 600: train loss 1.5975, val loss 1.7524\n",
      "iter 600: loss 1.6490, time 2391.64ms\n",
      "iter 610: loss 1.7271, time 99.09ms\n",
      "iter 620: loss 1.6379, time 76.27ms\n",
      "step 630: train loss 1.5883, val loss 1.7410\n",
      "iter 630: loss 1.5920, time 2333.74ms\n",
      "iter 640: loss 1.5814, time 75.91ms\n",
      "iter 650: loss 1.6327, time 76.30ms\n",
      "step 660: train loss 1.6004, val loss 1.7216\n",
      "saving checkpoint to instructions\n",
      "iter 660: loss 1.6647, time 2400.62ms\n",
      "iter 670: loss 1.5101, time 83.20ms\n",
      "iter 680: loss 1.6048, time 83.52ms\n",
      "step 690: train loss 1.5734, val loss 1.7279\n",
      "iter 690: loss 1.6561, time 2359.63ms\n",
      "iter 700: loss 1.7752, time 92.48ms\n",
      "iter 710: loss 1.5746, time 75.95ms\n",
      "step 720: train loss 1.5699, val loss 1.7283\n",
      "iter 720: loss 1.6300, time 2325.85ms\n",
      "iter 730: loss 1.6193, time 76.51ms\n",
      "iter 740: loss 1.6099, time 76.23ms\n",
      "step 750: train loss 1.5657, val loss 1.7181\n",
      "saving checkpoint to instructions\n",
      "iter 750: loss 1.5668, time 2339.88ms\n",
      "iter 760: loss 1.6806, time 76.40ms\n",
      "iter 770: loss 1.7067, time 75.59ms\n",
      "step 780: train loss 1.5658, val loss 1.7402\n",
      "iter 780: loss 1.7014, time 2270.03ms\n",
      "iter 790: loss 1.5630, time 75.94ms\n",
      "iter 800: loss 1.5903, time 75.96ms\n",
      "step 810: train loss 1.5598, val loss 1.7307\n",
      "iter 810: loss 1.6201, time 2270.66ms\n",
      "iter 820: loss 1.5829, time 75.55ms\n",
      "iter 830: loss 1.6767, time 76.37ms\n",
      "step 840: train loss 1.5856, val loss 1.7225\n",
      "iter 840: loss 1.5574, time 2273.53ms\n",
      "iter 850: loss 1.5963, time 76.20ms\n",
      "iter 860: loss 1.7630, time 75.75ms\n",
      "step 870: train loss 1.5675, val loss 1.7389\n",
      "iter 870: loss 1.6097, time 2271.54ms\n",
      "iter 880: loss 1.6667, time 76.04ms\n",
      "iter 890: loss 1.6450, time 75.80ms\n",
      "step 900: train loss 1.5448, val loss 1.7458\n",
      "iter 900: loss 1.5804, time 2273.00ms\n",
      "iter 910: loss 1.6201, time 75.71ms\n",
      "iter 920: loss 1.6476, time 76.09ms\n",
      "step 930: train loss 1.5603, val loss 1.7064\n",
      "saving checkpoint to instructions\n",
      "iter 930: loss 1.4925, time 2335.19ms\n",
      "iter 940: loss 1.5821, time 75.68ms\n",
      "iter 950: loss 1.5825, time 75.69ms\n",
      "step 960: train loss 1.5445, val loss 1.7232\n",
      "iter 960: loss 1.5906, time 2271.99ms\n",
      "iter 970: loss 1.6422, time 76.11ms\n",
      "iter 980: loss 1.5884, time 75.98ms\n",
      "step 990: train loss 1.5367, val loss 1.7320\n",
      "iter 990: loss 1.6491, time 2269.67ms\n",
      "iter 1000: loss 1.6726, time 75.79ms\n",
      "iter 1010: loss 1.6498, time 76.09ms\n",
      "step 1020: train loss 1.5231, val loss 1.7204\n",
      "iter 1020: loss 1.6303, time 2269.17ms\n",
      "iter 1030: loss 1.6116, time 75.98ms\n",
      "iter 1040: loss 1.7303, time 76.12ms\n",
      "step 1050: train loss 1.5317, val loss 1.7267\n",
      "iter 1050: loss 1.5553, time 2272.93ms\n",
      "iter 1060: loss 1.4864, time 75.80ms\n",
      "iter 1070: loss 1.6658, time 75.83ms\n",
      "step 1080: train loss 1.5259, val loss 1.7506\n",
      "iter 1080: loss 1.6860, time 2267.74ms\n",
      "iter 1090: loss 1.5386, time 75.78ms\n",
      "iter 1100: loss 1.7939, time 75.95ms\n",
      "step 1110: train loss 1.5128, val loss 1.7358\n",
      "iter 1110: loss 1.6122, time 2270.74ms\n",
      "iter 1120: loss 1.5815, time 76.00ms\n",
      "iter 1130: loss 1.6625, time 76.08ms\n",
      "step 1140: train loss 1.5152, val loss 1.7236\n",
      "iter 1140: loss 1.7641, time 2271.98ms\n",
      "iter 1150: loss 1.6649, time 75.85ms\n",
      "iter 1160: loss 1.5553, time 75.86ms\n",
      "step 1170: train loss 1.5421, val loss 1.7321\n",
      "iter 1170: loss 1.5080, time 2271.29ms\n",
      "iter 1180: loss 1.7058, time 75.95ms\n",
      "iter 1190: loss 1.5455, time 75.81ms\n",
      "step 1200: train loss 1.5092, val loss 1.7398\n",
      "iter 1200: loss 1.8617, time 2269.10ms\n",
      "iter 1210: loss 1.6061, time 76.00ms\n",
      "iter 1220: loss 1.5343, time 75.73ms\n",
      "step 1230: train loss 1.5173, val loss 1.7576\n",
      "iter 1230: loss 1.6338, time 2269.74ms\n",
      "iter 1240: loss 1.6208, time 75.97ms\n",
      "iter 1250: loss 1.6138, time 76.06ms\n",
      "step 1260: train loss 1.5218, val loss 1.7305\n",
      "iter 1260: loss 1.5319, time 2285.90ms\n",
      "iter 1270: loss 1.6055, time 75.97ms\n",
      "iter 1280: loss 1.6889, time 76.37ms\n",
      "step 1290: train loss 1.4958, val loss 1.7487\n",
      "iter 1290: loss 1.6941, time 2276.81ms\n",
      "iter 1300: loss 1.5098, time 76.54ms\n",
      "iter 1310: loss 1.6064, time 76.53ms\n",
      "step 1320: train loss 1.4995, val loss 1.7351\n",
      "iter 1320: loss 1.5690, time 2282.28ms\n",
      "iter 1330: loss 1.6078, time 76.64ms\n",
      "iter 1340: loss 1.5743, time 76.77ms\n",
      "step 1350: train loss 1.4996, val loss 1.7411\n",
      "iter 1350: loss 1.5564, time 2280.92ms\n",
      "iter 1360: loss 1.5510, time 76.45ms\n",
      "iter 1370: loss 1.6608, time 76.40ms\n",
      "step 1380: train loss 1.4703, val loss 1.7232\n",
      "iter 1380: loss 1.5925, time 2282.02ms\n",
      "iter 1390: loss 1.5422, time 76.43ms\n",
      "iter 1400: loss 1.6109, time 76.51ms\n",
      "step 1410: train loss 1.5017, val loss 1.7325\n",
      "iter 1410: loss 1.5534, time 2281.12ms\n",
      "iter 1420: loss 1.5274, time 76.02ms\n",
      "iter 1430: loss 1.4467, time 76.33ms\n",
      "step 1440: train loss 1.4832, val loss 1.7205\n",
      "iter 1440: loss 1.4788, time 2285.06ms\n",
      "iter 1450: loss 1.5560, time 76.15ms\n",
      "iter 1460: loss 1.6578, time 76.39ms\n",
      "step 1470: train loss 1.4685, val loss 1.7210\n",
      "iter 1470: loss 1.5540, time 2281.09ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1480: loss 1.6443, time 76.10ms\n",
      "iter 1490: loss 1.6027, time 76.14ms\n",
      "step 1500: train loss 1.4708, val loss 1.7382\n",
      "iter 1500: loss 1.4790, time 2280.57ms\n",
      "iter 1510: loss 1.4617, time 76.44ms\n",
      "iter 1520: loss 1.7106, time 76.34ms\n",
      "step 1530: train loss 1.4871, val loss 1.7315\n",
      "iter 1530: loss 1.4685, time 2277.61ms\n",
      "iter 1540: loss 1.5211, time 76.76ms\n",
      "iter 1550: loss 1.5933, time 76.26ms\n",
      "step 1560: train loss 1.4736, val loss 1.7385\n",
      "iter 1560: loss 1.5108, time 2376.81ms\n",
      "iter 1570: loss 1.5084, time 75.92ms\n",
      "iter 1580: loss 1.5839, time 76.41ms\n",
      "step 1590: train loss 1.4852, val loss 1.7570\n",
      "iter 1590: loss 1.5455, time 2403.54ms\n",
      "iter 1600: loss 1.6572, time 84.07ms\n",
      "iter 1610: loss 1.5415, time 76.96ms\n",
      "step 1620: train loss 1.4839, val loss 1.7462\n",
      "iter 1620: loss 1.5211, time 2286.64ms\n",
      "iter 1630: loss 1.5504, time 76.46ms\n",
      "iter 1640: loss 1.5573, time 76.51ms\n",
      "step 1650: train loss 1.4764, val loss 1.7451\n",
      "iter 1650: loss 1.7358, time 2399.22ms\n",
      "iter 1660: loss 1.5677, time 84.72ms\n",
      "iter 1670: loss 1.4395, time 76.35ms\n",
      "step 1680: train loss 1.4606, val loss 1.7518\n",
      "iter 1680: loss 1.5551, time 2307.53ms\n",
      "iter 1690: loss 1.5162, time 75.75ms\n",
      "iter 1700: loss 1.5960, time 76.42ms\n",
      "step 1710: train loss 1.4616, val loss 1.7251\n",
      "iter 1710: loss 1.7003, time 2268.37ms\n",
      "iter 1720: loss 1.5351, time 76.05ms\n",
      "iter 1730: loss 1.6316, time 76.08ms\n",
      "step 1740: train loss 1.4444, val loss 1.7467\n",
      "iter 1740: loss 1.4571, time 2272.00ms\n",
      "iter 1750: loss 1.4829, time 76.55ms\n",
      "iter 1760: loss 1.6180, time 75.95ms\n",
      "step 1770: train loss 1.4542, val loss 1.7510\n",
      "iter 1770: loss 1.5137, time 2274.42ms\n",
      "iter 1780: loss 1.6347, time 75.62ms\n",
      "iter 1790: loss 1.5037, time 76.02ms\n",
      "step 1800: train loss 1.4472, val loss 1.7716\n",
      "iter 1800: loss 1.4447, time 2300.84ms\n",
      "iter 1810: loss 1.5370, time 75.78ms\n",
      "iter 1820: loss 1.4937, time 75.63ms\n",
      "step 1830: train loss 1.4504, val loss 1.7556\n",
      "iter 1830: loss 1.5353, time 2279.55ms\n",
      "iter 1840: loss 1.5081, time 76.05ms\n",
      "iter 1850: loss 1.4940, time 76.84ms\n",
      "step 1860: train loss 1.4473, val loss 1.7467\n",
      "iter 1860: loss 1.3949, time 2284.76ms\n",
      "iter 1870: loss 1.4270, time 76.15ms\n",
      "iter 1880: loss 1.4560, time 77.23ms\n",
      "step 1890: train loss 1.4482, val loss 1.7417\n",
      "iter 1890: loss 1.4787, time 2299.61ms\n",
      "iter 1900: loss 1.6412, time 76.71ms\n",
      "iter 1910: loss 1.4933, time 80.65ms\n",
      "step 1920: train loss 1.4418, val loss 1.7440\n",
      "iter 1920: loss 1.5605, time 2280.76ms\n",
      "iter 1930: loss 1.4838, time 82.04ms\n",
      "iter 1940: loss 1.3840, time 76.06ms\n",
      "step 1950: train loss 1.4335, val loss 1.7574\n",
      "iter 1950: loss 1.5956, time 2288.94ms\n",
      "iter 1960: loss 1.5834, time 76.20ms\n",
      "iter 1970: loss 1.4951, time 76.22ms\n",
      "step 1980: train loss 1.4450, val loss 1.7630\n",
      "iter 1980: loss 1.3780, time 2283.67ms\n",
      "iter 1990: loss 1.4815, time 76.48ms\n",
      "iter 2000: loss 1.5108, time 76.69ms\n"
     ]
    }
   ],
   "source": [
    "#scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    logits, loss = model(X, Y)\n",
    "    X, Y = get_batch('train')\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #scaler.scale(loss).backward()\n",
    "    loss.backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        #scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    #scaler.step(optimizer)\n",
    "    #scaler.update()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        lossf = loss.item()\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1724ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7063528299331665"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061a2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
