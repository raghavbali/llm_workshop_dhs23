{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ7lJK0y9gJO"
      },
      "source": [
        "# Retrieval Augmented LLM App\n",
        "<img src=\"../assets/module_5/rap_banner.jpeg\">\n",
        "\n",
        "We have covered quite some ground in terms of understanding and building components for:\n",
        "- Text Representation\n",
        "- NLP Tasks\n",
        "- Pretrained Models and Transfer Learning\n",
        "- Model Fine-Tuning PEFT\n",
        "- SFT and LLM Landscape\n",
        "- Vector Databases\n",
        "- Libraries and Frameworks\n",
        "\n",
        "Now we will work through development of an app to showcase how we can leverage all the concepts into a fully functioning system\n",
        "\n",
        "__Note__: In order to keep things simple, we will leverage most high-level APIs available but the overall setup should be easily extensible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRjKaov9gJR"
      },
      "source": [
        "## Why Retrieval Augmentation\n",
        "\n",
        "While theoretically LLMs are capable of having super long context windows, in real world settings this is a challenge because of:\n",
        "- Inability/Limitation to ensure LLM focusses on correct sub-sections of the context\n",
        "- High Memory requirements\n",
        "- High API Cost\n",
        "- High Latency , etc.\n",
        "\n",
        "\n",
        "In order to overcome such challenges, we leverage vector databases to act as intelligent retrieval systems (again powered by LLMs) to:\n",
        "- Provide focussed context\n",
        "- Reduce memory, cost and latency requirements\n",
        "- Unlock super-abilities to use upto-date information\n",
        "- Offload trivial tasks to expert systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxi-yR1n9gJR"
      },
      "source": [
        "## Streamlit Enters the Arena\n",
        "\n",
        "[Streamlit](https://streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_05/2.retrieval_augmented_llm_app.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTFt0rOY9gJR"
      },
      "source": [
        "## Let us Begin with Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z_dNazilzRUF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# install dependencies\n",
        "!pip install -q chromadb\n",
        "!pip install retry\n",
        "!pip install -q streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjSlib36bJ70",
        "outputId": "9ccfe431-7738-459d-9d4f-b977c7fc7a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "## import required components\n",
        "\n",
        "from utils import (\n",
        "    get_lines,\n",
        "    load_data,\n",
        "    get_relevant_documents,\n",
        "    get_answer,\n",
        "    create_db,\n",
        "    sidebar\n",
        ")\n",
        "import streamlit as st\n",
        "chroma_client, db = create_db()\n",
        "\n",
        "## Setup Page Header and Sidebar\n",
        "st.set_page_config(page_title=\"PersonalGPT\", page_icon=\"ðŸ“–\", layout=\"wide\")\n",
        "st.header(\"ðŸ“–PersonalGPT\")\n",
        "sidebar()\n",
        "\n",
        "## Add Uploader Component\n",
        "uploaded_file = st.file_uploader(\n",
        "    \"Upload a txt file\",\n",
        "    type=[\"txt\"],\n",
        "    help=\"Text files with each sentence acting as a document\",\n",
        ")\n",
        "\n",
        "## Check if upload is complete\n",
        "if not uploaded_file:\n",
        "    st.stop()\n",
        "\n",
        "## Read uploaded file\n",
        "try:\n",
        "    file_data = get_lines(uploaded_file)\n",
        "    ## Verbose Status update\n",
        "    st.markdown(f\"> Uploaded file has {len(file_data)} lines of text\")\n",
        "except Exception as e:\n",
        "    st.markdown(f\"Could not upload/read file={e}\")\n",
        "\n",
        "## Index Uploaded text file\n",
        "with st.spinner(\"Indexing document... This may take a whileâ³\"):\n",
        "    load_data(db, documents=file_data)\n",
        "\n",
        "## status update\n",
        "st.markdown(f\"> Database indexed {db.count()} documents\")\n",
        "\n",
        "## Get User Input\n",
        "with st.form(key=\"qa_form\"):\n",
        "    query = st.text_area(\"Enter Your Query\")\n",
        "    submit = st.form_submit_button(\"Submit\")\n",
        "\n",
        "## Provide additional Options for citing source\n",
        "with st.expander(\"Advanced Options\"):\n",
        "    show_source = st.checkbox(\"Show Source\")\n",
        "\n",
        "## Generate Output upon button click\n",
        "if submit:\n",
        "  # Get relevant documents from DB\n",
        "  context = get_relevant_documents(query, db)\n",
        "\n",
        "  # get answer from LLM\n",
        "  answer,score,error = get_answer(query,context)\n",
        "\n",
        "  # Showcase response on screen\n",
        "  st.markdown(\"#### Answer\")\n",
        "  st.markdown(answer)\n",
        "  st.markdown(f\"> relevance score:{score}\")\n",
        "  st.markdown(\"---\")\n",
        "\n",
        "  # Add more details if advanced option is chosen\n",
        "  if show_source:\n",
        "    st.markdown(\"> Source(s):\")\n",
        "    st.markdown(f\"<i>{context[:100]}...</i>\", unsafe_allow_html=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mlGSHYN0bQSm"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5dllFNabXhE",
        "outputId": "8dd09f2d-c36d-49f1-e25d-991b18d1574c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.41s\n",
            "your url is: https://icy-heads-enjoy.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofMwZUzHBpEJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
