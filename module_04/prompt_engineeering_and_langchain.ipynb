{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfJ4nkINvlan"
   },
   "source": [
    "# Prompt Engineering\n",
    "<img src=\"../assets/module_4/pe_banner.jpg\">\n",
    "\n",
    "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
    "\n",
    "As a prompt engineer, you'll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn't about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\n",
    "\n",
    "In this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\n",
    "\n",
    "## Local Model using GPT4ALL\n",
    "> GPT4All is an open-source software ecosystem that allows anyone to train and deploy powerful and customized large language models (LLMs) on everyday hardware. Nomic AI oversees contributions to the open-source ecosystem ensuring quality, security and maintainability.\n",
    "\n",
    "It provides easy to setup and use python bindings.\n",
    "\n",
    "```python\n",
    "!pip install gpt4all\n",
    "```\n",
    "\n",
    "For OpenAI bindings\n",
    "```python\n",
    "!pip install --upgrade openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_04/prompt_engineeering_and_langchain.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWmkCGvUfu2y",
    "outputId": "b63ed1ce-846d-4c20-ea16-1fa3b4f9cebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-1.0.6-py3-none-macosx_10_9_universal2.whl (7.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.7 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from gpt4all) (2.28.0)\n",
      "Requirement already satisfied: tqdm in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from gpt4all) (4.64.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests->gpt4all) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests->gpt4all) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests->gpt4all) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests->gpt4all) (2.0.12)\n",
      "Installing collected packages: gpt4all\n",
      "Successfully installed gpt4all-1.0.6\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/r.bali/.pyenv/versions/3.8.11/envs/exp/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nAV5vV3Nfpej"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gpt4all\n",
    "from IPython.display import display, Markdown\n",
    "import openai\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_Y4Voy_Gfpel"
   },
   "outputs": [],
   "source": [
    "# NOTE: If you have access to openAI, this can be easily used with the same\n",
    "MODEL_TYPE = \"openLLAMA\"\n",
    "#or \"OPENAI\" #openLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVvFsppDfpel",
    "outputId": "5d25735d-1eae-4888-e643-8c296ce27aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[83064]: Class GGMLMetalClass is implemented in both /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x1110bf208) and /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x111227208). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: loading model from /Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9031.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "if MODEL_TYPE == \"OPENAI\":\n",
    "    API_KEY = \"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "    openai.organization = \"\"\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    # llama quantized\n",
    "    MODEL_NAME = \"nous-hermes-13b.ggmlv3.q4_0.bin\"\n",
    "    #or \"GPT4All-13B-snoozy.ggmlv3.q4_0.bin\"\n",
    "    llm_model = gpt4all.GPT4All(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yw30aS5vfpem"
   },
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"OPENAI\":\n",
    "    def get_completion(prompt, model):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages = messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "else:\n",
    "    def get_completion(prompt, model):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = model.generate(\n",
    "            prompt, streaming = False\n",
    "        )\n",
    "        return json.dumps(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MejaM6ov_jG"
   },
   "source": [
    "## Prompting Basics\n",
    "\n",
    "+ Be Clear and Provide Specific Instructions\n",
    "+ Allow Time to **Think**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "86fchLXjvkuz",
    "outputId": "3e2f07f5-7c33-469b-dd8f-68863f721fee"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the Transformer, a new network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional neural networks, and shows that it outperforms existing models in machine translation tasks in terms of quality, parallelizability, and training time.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific\n",
    "\n",
    "# Example: Clearly state which text to look at, provide delimiters\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence. Identify key contributions.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(get_completion(prompt, llm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output openLLAMA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key contributions: Propose a new simple network architecture called Transformer based solely on attention mechanisms, showing superiority over existing complex recurrent or convolutional neural networks while being faster and more efficient during training.\"\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(get_completion(prompt, llm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Summary: \n",
       "The Transformer network architecture, which is based solely on attention mechanisms and does not use recurrence or convolutions, outperforms other models in terms of quality, parallelizability, and training time on machine translation tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Be Clear and Specific\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence. Provide response in markdown format\n",
    "with a title for the summary.\n",
    "```{text}```\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output openLLAMA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"Title: Summary of the Transformer architecture for sequence transduction\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wz4xbjPHwmI_",
    "outputId": "f084df1e-fa42-4e6c-e6c4-a6fd916e1efe"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1 - To make tea, you first need to have a cup full of water, half cup milk, some sugar, and tea leaves.\n",
      "Point 2 - Start by boiling water.\n",
      "Point 3 - Once it comes to a boil, add milk to it.\n",
      "Point 4 - The next step is to add tea and let it boil for another minute.\n",
      "Point 5 - Add sugar to taste.\n",
      "Point 6 - Serve in a tall glass.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific, aka provide step by step instructions\n",
    "text = \"\"\"To make tea you first need to have a cup full of water,\n",
    "half cup milk, some sugar and tea leaves. Start by boiling water.\n",
    "Once it comes to a boil, add milk to it. Next step is to add tea and\n",
    "let it boil for another minute.\n",
    "Add sugar to taste. Serve in a tall glass\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Read the text delimited by triple single quotes.\n",
    "Check if it contains a sequence of instructions, \\\n",
    "re-write the instructions in the following format:\n",
    "\n",
    "Point 1 - ...\n",
    "Point 2 - …\n",
    "…\n",
    "Point N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\n",
    "then apologize that you cannot rephrase such text.\n",
    "\n",
    "'''{text}'''\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnKvLOjGfpep"
   },
   "source": [
    "> sample output openLLAMA\n",
    "\n",
    "Point 1 - Boil the water\n",
    "\n",
    "Point 2 - Add half cup of milk\n",
    "\n",
    "Point 3 - Add some sugar\n",
    "\n",
    "Point 4 - Let it boil for one more minute\n",
    "\n",
    "Point 5 - Add tea leaves and let it steep for another five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AI0x-syTfpep",
    "outputId": "b0c506e6-45de-4b91-e247-a0aa722afb83"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Snakes are elongated, legless reptiles belonging to the suborder Serpentes. They are characterized by their long, cylindrical bodies covered in scales, lack of limbs, and ability to move in a serpentine motion. Snakes are found in various habitats worldwide, except in Antarctica, Iceland, Ireland, and New Zealand. They come in a wide range of sizes, from tiny thread snakes measuring a few inches to large pythons and anacondas that can exceed 20 feet in length. Snakes are carnivorous and feed on a variety of prey, including rodents, birds, amphibians, and other reptiles. They have a unique way of capturing and consuming their food, using their highly flexible jaws to swallow prey whole. Some snakes are venomous, possessing specialized fangs and venom glands to immobilize or kill their prey, while others are non-venomous and rely on constriction to subdue their victims. Snakes play important roles in ecosystems as both predators and prey, and they have been the subject of fascination and fear throughout human history.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without instructions\n",
    "# openAI\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "get_completion('What are snakes?',llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IWcchnQKxgvX",
    "outputId": "e8603a5e-6e1e-421c-9f44-c31dfbaef702"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<father>: Snakes are long, slithery reptiles.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific, aka provide examples\n",
    "prompt = f\"\"\"\n",
    "Your task is to answer in conversation style mentioned in triple back quotes.\n",
    "Keep answers very short similar to examples provided below.\n",
    "\n",
    "```\n",
    "<kid>: What are birds?\n",
    "<father>: birds are cute little creatures that can fly\n",
    "\n",
    "<kid>: What are whales?\n",
    "<father>: Whales are very big fish that roam the oceans\n",
    "```\n",
    "\n",
    "<kid>: What are snakes?\n",
    "\"\"\"\n",
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S973gzC8lrzC"
   },
   "source": [
    "> sample output openLLAMA\n",
    "\n",
    "```\n",
    "<parent>: Snakes are slimy, scaly animals with no legs. They eat small creatures and can swallow them whole!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7Lc-_zCuyElX",
    "outputId": "4d3ee791-a751-44de-a876-44d78b0132a4"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The text mentions that the last holiday was in Germany and the cities visited were Berlin and Hamburg.\n",
      "\n",
      "German Translation: Unser letzter Urlaub war in Deutschland. Wir haben Berlin und Hamburg besucht.\n",
      "\n",
      "City List: Berlin, Hamburg\n",
      "\n",
      "Python Dictionary:\n",
      "{\n",
      "  \"original_text\": \"Our last holiday was in Germany. We visited Berlin and Hamburg.\",\n",
      "  \"german_translation\": \"Unser letzter Urlaub war in Deutschland. Wir haben Berlin und Hamburg besucht.\",\n",
      "  \"num_cities\": 2,\n",
      "  \"city_names\": [\"Berlin\", \"Hamburg\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Allow for time to think (similar to step by step instructions)\n",
    "text = \"\"\"\n",
    "Our last holiday was in Germany. We visited Berlin and Hamburg.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple \\\n",
    "backticks briefly. Then follow the instructions :\n",
    "1 - Translate the summary to German.\n",
    "2 - List each city in the text.\n",
    "3 - Output a python dictionary object that contains the following \\\n",
    "keys: original_text, german_translation, num_cities, city_names.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output openLLAMA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Summary: Our trip to Germany included visits to Berlin and Hamburg.\"\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LTg-YuwwyqgB",
    "outputId": "44613e8e-1133-4663-930d-186aa11dfa09"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I started with 10 apples.\n",
      "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
      "3. Then I bought 5 more apples, so now I have 11 apples.\n",
      "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
      "\n",
      "Is the user's solution the same as actual solution just calculated:\n",
      "yes\n",
      "\n",
      "Final Answer:\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "# Allow time to think, aka ask LLM to generate its own answer and then compare\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine if the user's solution delimited by triple back ticks\\\n",
    "is correct or not.\n",
    "To solve the problem the instructions are as follows:\n",
    "- Step 1: prepare your own solution to the problem.\n",
    "- Step 2: Compare your solution to the user's solution \\\n",
    "and evaluate if the user's solution is correct or not.\n",
    "Do not decide if the solution is correct until\n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "User's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the user's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Final Answer:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I went to the market and bought 10 apples.\n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "```\n",
    "User's solution:\n",
    "```\n",
    "1. I started with 10 apples.\n",
    "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
    "3. Then I bought 5 more apples, so now I have 11 apples.\n",
    "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
    "```\n",
    "Actual Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output openLLAMA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"```\\nYes, the user's solution is correct as it follows the given steps and arrives at the same answer as calculated above. So, the final number of apples remaining with you would be 10. The answer to the question \\\"How many apples did I remain with?\\\" is incorrect.\"\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt,llm_model)\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRyn_X282b0L"
   },
   "source": [
    "## Types of Prompts\n",
    "\n",
    "<img src=\"../assets/module_4/pe_types.jpg\">\n",
    "\n",
    "### Zero-Shot Prompting\n",
    "Zero-shot or without any examples. Since LLMs are trained on huge amounts of data and instructions, they work pretty well without any specific examples (shots) for usual tasks such as summarization, sentiment classification, grammar checks, etc.\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "Classify the text as neutral, positive or negative.\n",
    "Text: The food at this restaurant is so bad.\n",
    "Sentiment:\n",
    "\n",
    "```\n",
    "\n",
    "### Few-Shot Prompting\n",
    "LLMs are good for basic instructions they are trained with but for complex requirements they need some hand-holding or some examples to better understand the instructions.\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "Superb drinks and amazing service! > Positive\n",
    "I don't understand why this place is so expensive, worst food ever. > Negative\n",
    "Totally worth it, tasty 100%. > Positive\n",
    "This place is such an utter waste of time. >\n",
    "```\n",
    "**Note**: We did not explicitly instruct our LLM to do sentiment classification, rather gave examples (few-shot) to help it understand\n",
    "\n",
    "\n",
    "### Chain of Thought (COT)\n",
    "Tasks which are more complex and require a bit of *reasoning* (careful there 😉 ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\n",
    "\n",
    "_Sample Prompt_:\n",
    "<img src=\"../assets/module_4/cot_few_shot.png\">\n",
    "\n",
    "> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)\n",
    "\n",
    "#### COT Zero Shot ✨\n",
    "Extension of COT setup where instead of providing examples on how to solve a problem, we explicitly state ``Let's think step by step``. This was introduced by [Kojima et. al.](https://arxiv.org/abs/2205.11916)\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "I went to the market and bought 10 apples.\n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\n",
    "```\n",
    "\n",
    "## Advanced Prompting Techniques\n",
    "Prompt Engineering or PE is an active area of research where new techniques\n",
    "are being explored every day. Some of these are:\n",
    "\n",
    "  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\n",
    "  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
    "  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
    "  - Augmented Generation/Retrieval\n",
    "  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\n",
    "  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1av5flhfper"
   },
   "source": [
    "## LangChain 🦜🔗\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing LLM powered applications.\n",
    "- It provides capabilities to connect LLMs to a number of different sources of data\n",
    "- Provides interfaces for language models to interact with external environment (aka _Agentic_)\n",
    "- Provides for required levels of abstractions to designing end to end applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBf5chEsmlud",
    "outputId": "612055cb-1f91-4478-da63-9485b2d464b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (0.0.240)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (1.4.46)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (0.5.13)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (1.23.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (1.10.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (2.28.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.7.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/r.bali/.pyenv/versions/3.8.11/envs/exp/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/r.bali/.pyenv/versions/3.8.11/envs/exp/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOkoVCawp1gV",
    "outputId": "077303a3-bb40-4e25-e701-3af0b51a975b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'systemPrompt': '',\n",
       " 'promptTemplate': '### Instruction:\\n{0}\\n### Response:\\n',\n",
       " 'order': 'c',\n",
       " 'md5sum': '4acc146dd43eb02845c233c29289c7c5',\n",
       " 'name': 'Hermes',\n",
       " 'filename': 'nous-hermes-13b.ggmlv3.q4_0.bin',\n",
       " 'filesize': '8136777088',\n",
       " 'requires': '2.4.7',\n",
       " 'ramrequired': '16',\n",
       " 'parameters': '13 billion',\n",
       " 'quant': 'q4_0',\n",
       " 'type': 'LLaMA',\n",
       " 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>',\n",
       " 'path': '/Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-IQGlLI24Ifm"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp_LEN2y52rh",
    "outputId": "e5220778-6f12-4e6c-8039-ad7657f5ba0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9031.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "llm = GPT4All(\n",
    "    model='/Users/r.bali/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin',#GPT4All-13B-snoozy.ggmlv3.q4_0.bin',\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wczL5z0mkzW",
    "outputId": "231bc4fc-738d-4e6b-8659-3749a9299291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /root/.cache/gpt4all/GPT4All-13B-snoozy.ggmlv3.q4_0.bin\n",
      "Prompt: What is the capital of Germany\n",
      " The capital of Germany is Berlin"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the capital of Germany',\n",
       " 'text': ' The capital of Germany is Berlin'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a friendly chatbot assistant that responds in a conversational\n",
    "manner to users questions. Keep the answers short, unless specifically\n",
    "asked by the user to elaborate on something.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "query = input(\"Prompt: \")\n",
    "llm_chain(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Conversation Buffer\n",
    "\n",
    "LangChain provides us with an easy to use interface to enable LLMs to refer to context/memory\n",
    "across multiple chains/calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BtgDe_qeCDZP"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PcRIaGe0CE5s"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08O0wZ1l5wbE",
    "outputId": "9f0e9fba-f1d8-416e-91c9-4b97324f45dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Human: Follow the instruction I specify as {instruction} on the\n",
      "    text I specify as <text>\n",
      "    My first instruction is {summarize text briefly} <We went on Holiday to     Germany. We visited Berlin and Hamburg\n",
      "Assistant:\u001b[0m\n",
      " Berlin and Hamburg are two cities in Germany that you visited during your holiday there.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Berlin and Hamburg are two cities in Germany that you visited during your holiday there.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(\n",
    "    human_input=\"\"\"Follow the instruction I specify as {instruction} on the\n",
    "    text I specify as <text>\n",
    "    My first instruction is {summarize text briefly} <We went on Holiday to \\\n",
    "    Germany. We visited Berlin and Hamburg\"\"\"\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "S1SYcJIT88UY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Human: Follow the instruction I specify as {instruction} on the\n",
      "    text I specify as <text>\n",
      "    My first instruction is {summarize text briefly} <We went on Holiday to     Germany. We visited Berlin and Hamburg\n",
      "AI:  Berlin and Hamburg are two cities in Germany that you visited during your holiday there.\n",
      "Human: {Translate summary in German}\n",
      "Assistant:\u001b[0m\n",
      "Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"{Translate summary in German}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ObJCxLXz8Sl2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Human: Follow the instruction I specify as {instruction} on the\n",
      "    text I specify as <text>\n",
      "    My first instruction is {summarize text briefly} <We went on Holiday to     Germany. We visited Berlin and Hamburg\n",
      "AI:  Berlin and Hamburg are two cities in Germany that you visited during your holiday there.\n",
      "Human: {Translate summary in German}\n",
      "AI: Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.\n",
      "Human: {Identify the name of cities}\n",
      "Assistant:\u001b[0m\n",
      " The two cities you visited during your holiday to Germany are Berlin and Hamburg.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " The two cities you visited during your holiday to Germany are Berlin and Hamburg.\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"{Identify the name of cities}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yMHDglyf8vAU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Human: Follow the instruction I specify as {instruction} on the\n",
      "    text I specify as <text>\n",
      "    My first instruction is {summarize text briefly} <We went on Holiday to     Germany. We visited Berlin and Hamburg\n",
      "AI:  Berlin and Hamburg are two cities in Germany that you visited during your holiday there.\n",
      "Human: {Translate summary in German}\n",
      "AI: Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.\n",
      "Human: {Identify the name of cities}\n",
      "AI:  The two cities you visited during your holiday to Germany are Berlin and Hamburg.\n",
      "Human: prepare a python dictionary with keys original_text, city_names, summary, german_summary\n",
      "Assistant:\u001b[0m\n",
      " {'original_text': 'We went on Holiday to Germany. We visited Berlin and Hamburg', 'city_names': ['Berlin','Hamburg'],'summarize text briefly':'Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " {'original_text': 'We went on Holiday to Germany. We visited Berlin and Hamburg', 'city_names': ['Berlin','Hamburg'],'summarize text briefly':'Das war eine schnelle Übersicht über unseren Urlaub nach Deutschland, wo wir Berlin und Hamburg besucht haben.'}\n"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(human_input=\"prepare a python dictionary with keys original_text, city_names, summary, german_summary\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond LangChain\n",
    "\n",
    "### [LlamaIndex](https://www.llamaindex.ai/)\n",
    "Similar to langchain, LlamaIndex provides utilities to extend the power of LLMs through various integrations for:\n",
    "    - Data ingestion\n",
    "    - Data Indexing\n",
    "    - Querying\n",
    "\n",
    "### [LangSmith](https://docs.smith.langchain.com/)\n",
    "Build production grade applications by providing tools & utilities for\n",
    "    - Debugging\n",
    "    - Testing\n",
    "    - Integrations\n",
    "    - Token Usage\n",
    "\n",
    "### [HuggingFace](https://huggingface.co/models?other=LLM)\n",
    "The defacto standard for not just LLMs but large models across NLP, Computer vision and more.\n",
    "Libraries such as ``transformers``, ``diffusers``, ``accelerate`` and more provide ease of working\n",
    "with deep learning models in pytorch/tensorflow. Huggingface now also provides ``model-cards`` and ``model-spaces``\n",
    "for hosting and executing models on cloud for free.\n",
    "\n",
    "## [LLM-Foundry](https://github.com/mosaicml/llm-foundry)\n",
    "Mosaic ML released their own GPT style models based on special features such as [Flash Attention](https://arxiv.org/pdf/2205.14135.pdf) & [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) for efficient/faster\n",
    "training along with ALiBi for extended context lengths (65k+ tokens). LLM-Foundary is a package built to assist their implementations\n",
    "for training and fine-tuning LLMs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svRWMXXq9FIU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
