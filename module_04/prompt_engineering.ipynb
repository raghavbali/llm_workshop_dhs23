{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfJ4nkINvlan"
      },
      "source": [
        "# Prompt Engineering\n",
        "<img src=\"../assets/module_4/pe_banner.jpg\">\n",
        "\n",
        "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
        "\n",
        "As a prompt engineer, you'll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn't about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\n",
        "\n",
        "In this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\n",
        "\n",
        "## Local Model using Lang-Chain\n",
        "> Add a brief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MejaM6ov_jG"
      },
      "source": [
        "## Prompting Basics\n",
        "\n",
        "+ Be Clear and Provide Specific Instructions\n",
        "+ Allow Time to **Think**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86fchLXjvkuz"
      },
      "outputs": [],
      "source": [
        "# Be Clear and Specific\n",
        "\n",
        "# Example: Clearly state which text to look at, provide delimiters\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQ-laBCwmSM"
      },
      "outputs": [],
      "source": [
        "# Be Clear and Specific\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence. Provide response in markdown format\n",
        "with a title for the summary.\n",
        "```{text}```\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz4xbjPHwmI_"
      },
      "outputs": [],
      "source": [
        "# Be Clear and Specific, aka provide step by step instructions\n",
        "prompt = f\"\"\"\n",
        "Read the text delimited by triple quotes.\n",
        "Check if it contains a sequence of instructions, \\\n",
        "re-write the instructions in the following format:\n",
        "\n",
        "Point 1 - ...\n",
        "Point 2 - â€¦\n",
        "â€¦\n",
        "Point N - â€¦\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then apologize that you cannot rephrase such text.\n",
        "\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWcchnQKxgvX"
      },
      "outputs": [],
      "source": [
        "# Be Clear and Specific, aka provide examples\n",
        "prompt = f\"\"\"\n",
        "Your task is to answer in a the following style.\n",
        "\n",
        "<student>: What are birds?\n",
        "\n",
        "<teacher>: birds are living creatures with hollow bones.\n",
        "Most birds can fly with the help of feathers.\n",
        "\n",
        "<child>: What are whales?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lc-_zCuyElX"
      },
      "outputs": [],
      "source": [
        "# Allow for time to think (similar to step by step instructions)\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "Summarize the text delimited by triple \\\n",
        "backticks briefly. Then follow the instructions :\n",
        "1 - Translate the summary into German.\n",
        "2 - List each city in the German summary.\n",
        "3 - Output a python dictionary object that contains the following \\\n",
        "keys: original_text, german_translation, num_cities, city_names.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTg-YuwwyqgB"
      },
      "outputs": [],
      "source": [
        "# Allow time to think, aka ask LLM to generate its own answer and then compare\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Determine if the user's solution delimited by triple back ticks\\\n",
        "is correct or not.\n",
        "To solve the problem the instructions are as follows:\n",
        "- Step 1: prepare your own solution to the problem.\n",
        "- Step 2: Compare your solution to the user's solution \\\n",
        "and evaluate if the user's solution is correct or not.\n",
        "Do not decide if the solution is correct until\n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "```\n",
        "question here\n",
        "```\n",
        "User's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the user's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Final Answer:\n",
        "```\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I went to the market and bought 10 apples.\n",
        "I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "```\n",
        "User's solution:\n",
        "```\n",
        "1. I started with 10 apples.\n",
        "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
        "3. Then I bought 5 more apples, so now I have 11 apples.\n",
        "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
        "```\n",
        "Actual Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRyn_X282b0L"
      },
      "source": [
        "## Types of Prompts\n",
        "\n",
        "<img src=\"../assets/module_4/pe_types.jpg\">\n",
        "\n",
        "### Zero-Shot Prompting\n",
        "Zero-shot or without any examples. Since LLMs are trained on huge amounts of data and instructions, they work pretty well without any specific examples (shots) for usual tasks such as summarization, sentiment classification, grammar checks, etc.\n",
        "\n",
        "_Sample Prompt_:\n",
        "```\n",
        "Classify the text as neutral, positive or negative.\n",
        "Text: The food at this restaurant is so bad.\n",
        "Sentiment:\n",
        "\n",
        "```\n",
        "\n",
        "### Few-Shot Prompting\n",
        "LLMs are good for basic instructions they are trained with but for complex requirements they need some hand-holding or some examples to better understand the instructions.\n",
        "\n",
        "_Sample Prompt_:\n",
        "```\n",
        "Superb drinks and amazing service! > Positive\n",
        "I don't understand why this place is so expensive, worst food ever. > Negative\n",
        "Totally worth it, tasty 100%. > Positive\n",
        "This place is such an utter waste of time. >\n",
        "```\n",
        "**Note**: We did not explicitly instruct our LLM to do sentiment classification, rather gave examples (few-shot) to help it understand\n",
        "\n",
        "\n",
        "### Chain of Thought (COT)\n",
        "Tasks which are more complex and require a bit of *reasoning* (careful there ðŸ˜‰ ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\n",
        "\n",
        "_Sample Prompt_:\n",
        "<img src=\"../assets/module_4/cot_few_shot.png\">\n",
        "\n",
        "> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)\n",
        "\n",
        "#### COT Zero Shot âœ¨\n",
        "Extension of COT setup where instead of providing examples on how to solve a problem, we explicitly state ``Let's think step by step``. This was introduced by [Kojima et. al.](https://arxiv.org/abs/2205.11916)\n",
        "\n",
        "_Sample Prompt_:\n",
        "```\n",
        "I went to the market and bought 10 apples.\n",
        "I gave 2 apples to the neighbor and 2 to the repairman.\n",
        "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\n",
        "```\n",
        "\n",
        "## Advanced Prompting Techniques\n",
        "Prompt Engineering or PE is an active area of research where new techniques\n",
        "are being explored every day. Some of these are:\n",
        "\n",
        "  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\n",
        "  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
        "  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
        "  - Augmented Generation/Retrieval\n",
        "  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\n",
        "  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
