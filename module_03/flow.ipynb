{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb1a2f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let us start by fetching data\n",
    "\n",
    "### go to notebook 1-GetData.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff6027",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let us now create the datasets and vocab for pre-training\n",
    "\n",
    "### go to notebook 2-PreparePre-TrainingDataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39633d62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPT (Generative Pre-Trained Transformer)\n",
    "\n",
    "<img src=\"../assets/module_3/gpt_architecture.png\" >\n",
    "\n",
    "source: https://commons.wikimedia.org/wiki/File:Full_GPT_architecture.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18799931",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go to model_exercise1.py\n",
    "- Go through the modules\n",
    "- Complete the tasks marked as \"to-do\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974913d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-train the character LM\n",
    "### Use notebook 3-TrainCharLM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ac322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Generate names\n",
    "### Use notebook 4-Sample.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5d49b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What did the model learn?\n",
    "### Explore that in 5-Embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93040b79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt the model\n",
    "- to generate names starting from 'a'\n",
    "- to generate names starting from 'kr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccc892",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Can you prompt the model to generate names ending with 'a'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862cbde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-tune the pre-trained model\n",
    "\n",
    "## task = endswith_a\n",
    "\n",
    "### Use notebook 6-PrepareFine-TuningDatasets.ipynb to generate all fine-tuning datasets\n",
    "\n",
    "### Then proceed to the trainer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9733d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Great !  We fined-tuned the pre-trained model on task - endswith_a\n",
    "\n",
    "### Try generating few such names from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a580865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## But is it efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be74d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let us fine-tune for a classification task\n",
    "\n",
    "## Gender Classification: Given a name, predict the gender from male or female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c838f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We do have a generative pre-trained model which is trained of names.\n",
    "\n",
    "It understands the structures of names, which should help us classify by \"transfering\" its learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffb782",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/task-agnostic_sft.png\">\n",
    "source=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb4f30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go to model_exercise2.py\n",
    "\n",
    "### Make the appropriate changes to the model and use the trainer to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa7a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Train the full network, or train only the classifier head\n",
    "- Compare the number of trainable parameters\n",
    "- Compare the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e9acf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adapter: a PEFT technique\n",
    "\n",
    "## What is an adapter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95dfe8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## light weight task specific module plugged at the end of each layer\n",
    "<img src=\"../assets/module_3/adapter.png\">\n",
    "source=https://aclanthology.org/D19-1165.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f0201",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Layer Norm is applied to the input to the adapter. This makes it pluggable anywhere irrespective of the variations in the activation distributions/patterns\n",
    "    - This parametrized normalization layer allows the module to learn the activa- tion pattern of the layer it’s injected into\n",
    "    \n",
    "- The inner dimension of these two projections is the only knob to tune\n",
    "    - This allows us to adjust the capacity of the adapter module easily, depending on the complexity of the target task\n",
    "    \n",
    "- Residual connection allows the adapter to represent a no-op if necessary\n",
    "- Multiple task-specific adapters can be traind simultaneously\n",
    "- During inference adapters can be plugged to transform the model into the task specific function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebd9f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go to model_exercise3.py\n",
    "\n",
    "### Make the appropriate changes to the model and use the trainer to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67861072",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Experiment with different adapter sizes for our gender classification task\n",
    "- Compare the parameter efficiency and the performance against the full model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f100e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Low Rank Adaptation (LoRA)\n",
    "\n",
    "## Inspiration\n",
    "Learned over-parametrized models in fact reside on a low intrinsic dimension\n",
    "\n",
    "Hence ..\n",
    "\n",
    "Change in weights during model adaptation also has a low “intrinsic rank”, leading to LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a29489",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/lora.png\">\n",
    "source=https://arxiv.org/pdf/2106.09685.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130469d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### The authors show that for GPT-3 175B, a very low rank of 1 or 2 suffices even when the full rank is as high as 12,288, making LoRA both storage and compute efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89503b13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d4edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LoRA advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89360b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Adapter add mode layers (even though they are small), which increase latency\n",
    "### Large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one.\n",
    "\n",
    "\n",
    "<img src=\"../assets/module_3/lora_vs_adapter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bb79d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## No additional inference latency\n",
    "\n",
    "### When deployed in production, we can explicitly compute and store W = W0 + BA and perform inference as usual\n",
    "\n",
    "### When we need to switch to another downstream task, we can recover Wo by subtracting BA and then adding a different B′A′, a quick operation with very little memory overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bdaff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/lora_equation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e483e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A Generalization of Full Fine-tuning\n",
    "\n",
    "### When applying LoRA to all weight matrices and training all biases, we roughly recover the expressiveness of full fine-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices\n",
    "\n",
    "### As we increase the number of trainable parameters, training LoRA roughly converges to training the original model, while adapter-based methods converges to an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07096c96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LoRA for GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292bd43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### There are two linear layers in Attention\n",
    "- one is used to project query, key and value matrices\n",
    "- other to project the output of attention\n",
    "\n",
    "### There are two linear layers in MLP\n",
    "\n",
    "### LoRA can be applied to any weight matrices to achieve parameter efficiency\n",
    "\n",
    "## Let us apply to these four linear layers' weights matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e2905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### The paper uses a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "\n",
    "### They then scale ∆W x by α/r , where α is a constant in r. Let us use α=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaff2cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Go to model_exercise4.py\n",
    "\n",
    "### Make the appropriate changes to the model and use the trainer to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad542b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Play Around\n",
    "\n",
    "Compare parameter efficiency\n",
    "\n",
    "Compare performance\n",
    "\n",
    "You can compare the latency too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b2bd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd33af8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## In-context learning via hard prompts\n",
    "\n",
    "<img src=\"../assets/module_3/icl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480b238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### hard prompt because we’re using actual tokens that are not differentiable (think of “hard” as something static or set in stone)\n",
    "\n",
    "### The problem here is that the output of our LLM is highly-dependent on how we constructed our prompt.\n",
    "\n",
    "### Hard prompts are compute hungry as much of the block/context is spent on them and much less is left for the actual \"context\" or the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c15de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## what if we can learn our prompts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00fbb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prompt and prefix tuning solves this by making use of soft prompts— a vector attached to the input embedding that we train while keeping the pretrained LM frozen\n",
    "\n",
    "### soft prompt, because they are differentiable continuous vectors, which we can optimize/learn\n",
    "\n",
    "### In prompt tuning we add soft prompts only at the input, and in prefix tuning we prefix soft prompts at each decoder layer\n",
    "\n",
    "### Today we will explore prompt-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af1479",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Prompt Tuning: a parameter efficient fine-tuning technique\n",
    "\n",
    "<img src=\"../assets/module_3/prompt_tuning.png\">\n",
    "\n",
    "source=https://aclanthology.org/2021.emnlp-main.243.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us experiment with prompt tuning using the task: last_char\n",
    "\n",
    "### Here, given a word, model needs to give the last character\n",
    "\n",
    "### We are only taking up one task, although like the original paper shows one can perform multi-task learning where prefix vectors are different for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef6a53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Config paramater \"prompt_vocab_size\" specifies total number of prefix vectors to be learned\n",
    "\n",
    "- In our case it is same as prefix vectors per task, because we have only one task\n",
    "\n",
    "### We treat this as language generation task and hence use the default \"lm_head\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bd55b31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An example input\n",
    "\n",
    "input =====>>  {   a   m   a   r   =   r   }\n",
    "target ====>>  *   *   *   *   *   r   }   *\n",
    "\n",
    "'=' prompts the model to generate the prediction for the task\n",
    "\n",
    "{amar0=r}\n",
    "The dataset files have '0' embedded to indicate the end of the part where we don't need any targets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "046998d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "assume block size as 12 and prompt_vocab_size as 3\n",
    "\n",
    "index    =====>>   0    1    2    3    4    5    6    7    8    9    10   11\n",
    "\n",
    "input    =====>>   p1   p2   p3   {    a    m    a    r    =    r    }    *\n",
    "\n",
    "target   =====>>   *    *    *    *    *    *    *    *    r    }    *    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c4c40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Thats it ! Tune your prompts\n",
    "\n",
    "### Go to model_exercise5.py and complete the tasks\n",
    "\n",
    "### train, compare, and sample from your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c3ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling of Prompt-tuning\n",
    "\n",
    "<img src=\"../assets/module_3/prompt_tuning_scaling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3101e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instruction Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3e39b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Can models learn to follow instructions?\n",
    "\n",
    "\n",
    "## Can models perform tasks described purely via instructions?\n",
    "\n",
    "\n",
    "## Can models perform unseen tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a76d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What is instruction tuning?\n",
    "\n",
    "<img src=\"../assets/module_3/instruction_tuning_overview.png\">\n",
    "\n",
    "source=https://arxiv.org/pdf/2109.01652.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d83da0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## How does instruction tuning compare against other methods?\n",
    "\n",
    "<img src=\"../assets/module_3/instruction_tuning_comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2863387",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using various ways to demonstrate a task\n",
    "\n",
    "<img src=\"../assets/module_3/instruction_tuning_templates.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling Laws\n",
    "\n",
    "<img src=\"../assets/module_3/instruction_tuning_scaling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80731f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### For smallers models, all their capacity is probably used to learn to do the shown tasks\n",
    "\n",
    "### Larger models use some of their capacity to perform the tasks. But they have some remaining capacity to learn to follow instructions too. This helps them generalize to new tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9263e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prompt-tuning works better on instruction-tuned models\n",
    "\n",
    "<img src=\"../assets/module_3/instruction_tuning_prompt_tuning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee1cc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## More so in low resource setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc354b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Role of Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f872606",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A possibility: performance gains are due to multi-task fine-tuning and not due to instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c2373",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/role_of_instructions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb832a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let us Instruction-tune our pre-trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33ca62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Startswith\n",
    "- St0{tanu}\n",
    "- Sgu0{gurleen}\n",
    "\n",
    "### Endswith\n",
    "- Edu0{paddu}\n",
    "- Ene0{arianne}\n",
    "- En0{parthiban}\n",
    "\n",
    "### Gender Classification\n",
    "- G{priyadarsini0=F}\n",
    "- G{naran0=M}\n",
    "\n",
    "### Indian Classification ('I' is Indian, 'O' is Other)\n",
    "- C{shafeeque0=I}\n",
    "- C{jullian0=O}\n",
    "- C{vineeta0=I}\n",
    "\n",
    "### 'S','E','G','C' are the instructions\n",
    "\n",
    "## Note that classification tasks are also converted to language modeling/generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165cfb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go ahead! Try it out!\n",
    "\n",
    "### Also use generate method to sample from model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a1bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ok enough with the baby models !!\n",
    "\n",
    "## I want to play with some large models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801cdcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# let us inject some Common Sense into the GPT2 models\n",
    "\n",
    "https://inklab.usc.edu/CommonGen/\n",
    "\n",
    "Run the script 7-GetCommongenData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0358e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go to 8-SampleGPT.ipynb\n",
    "- Try out differenet gpt models\n",
    "- Try out different prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebefb71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Go to model_exercise6.py and complete the tasks\n",
    "- Use trainer 9-Fine-TuneGPT.ipynb to prompt-tune\n",
    "- Sample from tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4624cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ccb3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Gargantuan datasets\n",
    "- The size of modern pre-training datasets renders it impractical for any individual to read or conduct quality assessments on the encompassed documents thoroughly\n",
    "- Datasets need to be cleaned and quality checked thoroughly before going into pre-training\n",
    "    - Cannot afford to train multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3853a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Near Duplicates\n",
    "    - degrade performance\n",
    "    - leads to memorization by models sometimes\n",
    "- Benchmark Data Contamination\n",
    "    - leads to inflation of performance metrics\n",
    "    - when chatGPT was asked generate instances of academic benchmarks, they found it had memorized some test splits too\n",
    "- Personally Identifiable Information\n",
    "    - models typically need to memorize for strong performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7653ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pre-training Domain Mixtures\n",
    "    - mixture benefecial for transferability and generazibility\n",
    "        - But what amount of data from different sources needed for better downstream performance\n",
    "    - heterogeneous data sources more important than data quality\n",
    "        - motivates smaller yet more diverse pre-training datasets\n",
    "- Fine-Tuning Task Mixtures\n",
    "    - How to balance tasks in multi-task tine-tuning setup?\n",
    "    - negative task transfer, where learning multiple tasks at once hinders the learning of some specific tasks\n",
    "    - catastrophic forgetting of previous tasks when learning new tasks\n",
    "    - right proportion depends on the downstream end goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e0437",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenizer Reliance\n",
    "- Generally, Unsupervised\n",
    "- more tokens (sub-word tokeniation) lead to computational complexity\n",
    "    - but necessary to rare and handle out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80b08c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Tight coupling between pre-training data and tokenizer\n",
    "- discrepancies between the data that a tokenizer and a model have been trained on can lead to glitch tokens\n",
    "    - cause unexpected model behavior as their corresponding embeddings are essentially untrained\n",
    "    - needs re-training of tokenizer when the pre-training corpus is changed\n",
    "- Different languages requires different amount of tokens needs to express the same meaning\n",
    "    - interoperability becomes a challenge in multi-lingual setting\n",
    "    - also this tends to become unfair as different languages use different size of prompts which consume part of context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6202db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Power Law of Scaling\n",
    "- model performances scale as a power law with model size, dataset size, and the amount of compute used for training\n",
    "- Unsustainable\n",
    "- state-of-the-art results are essentially “bought” by spending massive computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b559f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- when selecting a model size, the computation resources for later usage (inference) should be considered, not just the one-time training costs\n",
    "    - it is shown that many llms are undertrained\n",
    "- train a smaller model more intensively upfront to offset larger inference costs in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856fb77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-Training Objectives\n",
    "- choice of PTO heavily influences the model’s data efficiency during pre-training\n",
    "- which in turn can reduce the number of iterations required\n",
    "- pre-training objective is typically function of \n",
    "    - architecture\n",
    "    - input/targets constrution\n",
    "    - masking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab29cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/masking.png\">\n",
    "\n",
    "source=https://arxiv.org/pdf/2307.10169.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a588875",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelism Strategies\n",
    "- divide and conquery strategy\n",
    "- model parallelism\n",
    "    - waiting times\n",
    "    - underutilized resources\n",
    "- pipeline parallelism\n",
    "    - combine with data parallelism\n",
    "    - data divided into minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7e40a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-Tuning Overhead\n",
    "- Fine-tuning entire LLMs requires the same amount of memory as pre-training\n",
    "- When adapting an LLM via full-model fine-tuning, an individual copy of the model must be stored (consuming data storage) and loaded (expending memory allocation, etc.) for each task\n",
    "- Parameter-efficient fine-tuning of LLMs still requires computing full forward/back- ward passes throughout the whole network\n",
    "- Fine-tuning an LLM, even with PEFT methods, still requires full gradient computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924274ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# High Inference Latency\n",
    "- LLM inference latencies remain high because of low parallelizability and large memory footprints\n",
    "- Quantization helps a great deal\n",
    "- Mixture of Experts\n",
    "    - a set of experts (modules), each with unique weights\n",
    "    - a router (or gating) network, which determines which expert module processes an input\n",
    "- Cascading\n",
    "    - refers to the idea of employing differently-sized models for different queries\n",
    "- Decoding Strategies\n",
    "    - can greatly impact the computational cost of performing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab207b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limited Context Length\n",
    "- Limited context lengths are a barrier for handling long inputs well to facilitate applications like novel or textbook writing or summarizing\n",
    "- Length Generalization\n",
    "    - short lengths during training but should generalize to longer lengths during inference\n",
    "- Positional Embeddings play a big role in constraining or generalizing to different lengths\n",
    "    - AliBi\n",
    "    - Learned vs Fixed Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba58bbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt Brittleness\n",
    "- Models are very sensitive to syntax of prompts\n",
    "    - length\n",
    "    - blanks\n",
    "    - ordering of examples\n",
    "- Even semantics\n",
    "    - wording\n",
    "    - selection of examples\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1f38e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../assets/module_3/prompt_brittleness.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85614c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hallucinations\n",
    "- How to measure hallucinations?\n",
    "- we can distinguish between intrinsic and extrinsic hallucinations\n",
    "    - intrinsic: the generated text logically contradicts the source content\n",
    "    - extrinsic: we cannot verify the output correctness from the provided source\n",
    "- Retrieval Augmentation\n",
    "    - mitigates hallucinations by grounding model's input on external knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dba6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Misaligned Behavior\n",
    "- Harmful/abusive/toxic/biased content\n",
    "- Not aligned to user's query\n",
    "\n",
    "- Instruction Tuning\n",
    "- Using Human Feedback during pre-training/fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b1fa3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outdated Knowledge\n",
    "\n",
    "- Model Editing\n",
    "    - bug-fixing\n",
    "        - locate the bug\n",
    "        - apply the update\n",
    "    - meta-learning\n",
    "        - uses external model to update the weights\n",
    "- Retrieval Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745befd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Papers presenting novel LLMs often lack controlled experiments, likely due to the prohibitive costs of training enough models\n",
    "\n",
    "## Parallelism strategies designed to distribute the training process across many accelerators are typically non-deterministic, rendering LLM training irreproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a6f5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
