{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d42342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,math,pickle,random\n",
    "import numpy as np,pandas as pd\n",
    "import torch,torch._dynamo\n",
    "from model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ad3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1337\n",
    "out_dir = 'fine-tuning'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "model_dir='long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f27090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fed177",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e09fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.37M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(55, 216, padding_idx=27)\n",
       "    (wpe): Embedding(50, 216)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=216, out_features=648, bias=False)\n",
       "          (c_proj): Linear(in_features=216, out_features=216, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=216, out_features=864, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=864, out_features=216, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=216, out_features=55, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(model_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "config=checkpoint[\"config\"]\n",
    "model = GPT(config)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open(os.path.join(out_dir, 'train.bin'),\"r\")\n",
    "train_names=[]\n",
    "evrythng=fp.readlines()\n",
    "for line in evrythng:\n",
    "    train_names.append(line.split(\"\\n\")[0])\n",
    "fp.close()\n",
    "fp=open(os.path.join(out_dir, 'val.bin'),\"r\")\n",
    "val_names=[]\n",
    "evrythng=fp.readlines()\n",
    "for line in evrythng:\n",
    "    val_names.append(line.split(\"\\n\")[0])\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad53266",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "block_size=config[\"block_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b086a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = config[\"n_layer\"]\n",
    "n_head = config[\"n_head\"]\n",
    "n_embd = config[\"n_embd\"]\n",
    "dropout = config[\"dropout\"]\n",
    "bias=config[\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5516da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "max_iters = 1000\n",
    "lr_decay_iters = 1000\n",
    "min_lr = 1e-4\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "warmup_iters = 50\n",
    "grad_clip = 1.0\n",
    "decay_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74df2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 30\n",
    "log_interval = 10\n",
    "eval_iters = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e473602",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "dtype='float16'\n",
    "compile=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36887b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 400\n"
     ]
    }
   ],
   "source": [
    "tokens_per_iter =  batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401658cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c0313d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_names if split == 'train' else val_names\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    pad_token=stoi[\"*\"]\n",
    "    x=torch.ones(batch_size,block_size,dtype=torch.long)*pad_token\n",
    "    y=torch.ones(batch_size,block_size,dtype=torch.long)*pad_token\n",
    "    for i,index in enumerate(ix):\n",
    "        encoded=encode(data[index])\n",
    "        x[i][:len(encoded)-1]=torch.Tensor(encoded[:-1])\n",
    "        y[i][:len(encoded)-1]=torch.Tensor(encoded[1:])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644f16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 3,381,912 parameters\n",
      "num non-decayed parameter tensors: 13, with 2,808 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44287bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8ac3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de00bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8634, val loss 1.8713\n",
      "iter 0: loss 1.8983, time 831.98ms\n",
      "iter 10: loss 1.6585, time 45.48ms\n",
      "iter 20: loss 1.8507, time 44.80ms\n",
      "step 30: train loss 1.8305, val loss 1.7898\n",
      "saving checkpoint to fine-tuning\n",
      "iter 30: loss 1.7881, time 634.26ms\n",
      "iter 40: loss 1.8344, time 45.01ms\n",
      "iter 50: loss 1.7297, time 45.09ms\n",
      "step 60: train loss 1.7849, val loss 1.7722\n",
      "saving checkpoint to fine-tuning\n",
      "iter 60: loss 1.7819, time 608.30ms\n",
      "iter 70: loss 1.9582, time 44.65ms\n",
      "iter 80: loss 1.9889, time 44.78ms\n",
      "step 90: train loss 1.7696, val loss 1.8369\n",
      "iter 90: loss 1.5937, time 576.83ms\n",
      "iter 100: loss 1.5041, time 44.57ms\n",
      "iter 110: loss 1.7855, time 44.77ms\n",
      "step 120: train loss 1.7886, val loss 1.7887\n",
      "iter 120: loss 1.8312, time 610.09ms\n",
      "iter 130: loss 1.9452, time 45.23ms\n",
      "iter 140: loss 1.8681, time 44.33ms\n",
      "step 150: train loss 1.7524, val loss 1.8125\n",
      "iter 150: loss 1.9883, time 580.69ms\n",
      "iter 160: loss 1.8151, time 45.36ms\n",
      "iter 170: loss 1.8864, time 46.70ms\n",
      "step 180: train loss 1.7721, val loss 1.7587\n",
      "saving checkpoint to fine-tuning\n",
      "iter 180: loss 1.7906, time 606.85ms\n",
      "iter 190: loss 1.8865, time 44.36ms\n",
      "iter 200: loss 1.8703, time 44.87ms\n",
      "step 210: train loss 1.7243, val loss 1.7676\n",
      "iter 210: loss 1.5980, time 561.68ms\n",
      "iter 220: loss 1.8751, time 45.62ms\n",
      "iter 230: loss 1.5224, time 45.65ms\n",
      "step 240: train loss 1.7434, val loss 1.7340\n",
      "saving checkpoint to fine-tuning\n",
      "iter 240: loss 1.6359, time 616.43ms\n",
      "iter 250: loss 1.8829, time 45.12ms\n",
      "iter 260: loss 2.0562, time 44.73ms\n",
      "step 270: train loss 1.7314, val loss 1.7941\n",
      "iter 270: loss 1.7445, time 585.10ms\n",
      "iter 280: loss 1.5960, time 45.25ms\n",
      "iter 290: loss 2.1664, time 47.57ms\n",
      "step 300: train loss 1.7652, val loss 1.7775\n",
      "iter 300: loss 1.8739, time 543.54ms\n",
      "iter 310: loss 1.6848, time 45.32ms\n",
      "iter 320: loss 2.0412, time 45.59ms\n",
      "step 330: train loss 1.6887, val loss 1.7587\n",
      "iter 330: loss 1.7890, time 568.78ms\n",
      "iter 340: loss 1.6418, time 45.09ms\n",
      "iter 350: loss 1.7650, time 46.17ms\n",
      "step 360: train loss 1.7109, val loss 1.7570\n",
      "iter 360: loss 1.9224, time 556.66ms\n",
      "iter 370: loss 1.6197, time 45.60ms\n",
      "iter 380: loss 1.5082, time 45.18ms\n",
      "step 390: train loss 1.7580, val loss 1.7245\n",
      "saving checkpoint to fine-tuning\n",
      "iter 390: loss 2.1113, time 600.33ms\n",
      "iter 400: loss 2.1008, time 46.34ms\n",
      "iter 410: loss 2.0316, time 44.73ms\n",
      "step 420: train loss 1.7205, val loss 1.7587\n",
      "iter 420: loss 1.7339, time 541.51ms\n",
      "iter 430: loss 1.7690, time 45.60ms\n",
      "iter 440: loss 1.6822, time 44.90ms\n",
      "step 450: train loss 1.6861, val loss 1.7268\n",
      "iter 450: loss 1.8860, time 542.93ms\n",
      "iter 460: loss 1.8621, time 45.73ms\n",
      "iter 470: loss 1.7072, time 45.08ms\n",
      "step 480: train loss 1.7316, val loss 1.7412\n",
      "iter 480: loss 1.6058, time 543.22ms\n",
      "iter 490: loss 1.6412, time 45.43ms\n",
      "iter 500: loss 1.5587, time 45.61ms\n",
      "step 510: train loss 1.7368, val loss 1.7163\n",
      "saving checkpoint to fine-tuning\n",
      "iter 510: loss 1.8350, time 663.07ms\n",
      "iter 520: loss 1.6053, time 45.87ms\n",
      "iter 530: loss 1.9076, time 45.15ms\n",
      "step 540: train loss 1.6546, val loss 1.7359\n",
      "iter 540: loss 1.9096, time 549.87ms\n",
      "iter 550: loss 1.7815, time 47.43ms\n",
      "iter 560: loss 1.8698, time 45.29ms\n",
      "step 570: train loss 1.7014, val loss 1.7945\n",
      "iter 570: loss 1.7037, time 541.00ms\n",
      "iter 580: loss 1.9562, time 45.63ms\n",
      "iter 590: loss 1.8851, time 45.71ms\n",
      "step 600: train loss 1.6858, val loss 1.7109\n",
      "saving checkpoint to fine-tuning\n",
      "iter 600: loss 1.7411, time 607.32ms\n",
      "iter 610: loss 1.5467, time 45.34ms\n",
      "iter 620: loss 1.9790, time 45.68ms\n",
      "step 630: train loss 1.7220, val loss 1.7223\n",
      "iter 630: loss 1.9044, time 546.86ms\n",
      "iter 640: loss 1.9579, time 45.78ms\n",
      "iter 650: loss 1.7442, time 46.22ms\n",
      "step 660: train loss 1.6811, val loss 1.7672\n",
      "iter 660: loss 1.9284, time 543.14ms\n",
      "iter 670: loss 1.4536, time 44.61ms\n",
      "iter 680: loss 1.6189, time 44.64ms\n",
      "step 690: train loss 1.6794, val loss 1.7540\n",
      "iter 690: loss 1.7438, time 544.96ms\n",
      "iter 700: loss 1.5151, time 46.54ms\n",
      "iter 710: loss 1.7846, time 45.89ms\n",
      "step 720: train loss 1.6555, val loss 1.6956\n",
      "saving checkpoint to fine-tuning\n",
      "iter 720: loss 1.7951, time 618.89ms\n",
      "iter 730: loss 1.8126, time 45.84ms\n",
      "iter 740: loss 1.8286, time 46.83ms\n",
      "step 750: train loss 1.6801, val loss 1.6792\n",
      "saving checkpoint to fine-tuning\n",
      "iter 750: loss 1.4091, time 655.51ms\n",
      "iter 760: loss 1.6856, time 46.42ms\n",
      "iter 770: loss 1.7689, time 47.12ms\n",
      "step 780: train loss 1.7004, val loss 1.7702\n",
      "iter 780: loss 1.6114, time 559.06ms\n",
      "iter 790: loss 1.5969, time 45.08ms\n",
      "iter 800: loss 1.6593, time 44.80ms\n",
      "step 810: train loss 1.6449, val loss 1.8012\n",
      "iter 810: loss 1.7835, time 540.58ms\n",
      "iter 820: loss 1.6464, time 44.78ms\n",
      "iter 830: loss 1.6995, time 44.48ms\n",
      "step 840: train loss 1.6455, val loss 1.7827\n",
      "iter 840: loss 1.8304, time 576.21ms\n",
      "iter 850: loss 1.5328, time 44.46ms\n",
      "iter 860: loss 1.8441, time 44.65ms\n",
      "step 870: train loss 1.6609, val loss 1.6785\n",
      "saving checkpoint to fine-tuning\n",
      "iter 870: loss 1.9706, time 609.85ms\n",
      "iter 880: loss 1.6030, time 45.19ms\n",
      "iter 890: loss 1.6653, time 45.16ms\n",
      "step 900: train loss 1.6576, val loss 1.7307\n",
      "iter 900: loss 2.0528, time 542.61ms\n",
      "iter 910: loss 1.4017, time 44.31ms\n",
      "iter 920: loss 1.7213, time 44.77ms\n",
      "step 930: train loss 1.6492, val loss 1.6910\n",
      "iter 930: loss 1.7429, time 551.39ms\n",
      "iter 940: loss 1.5712, time 44.86ms\n",
      "iter 950: loss 1.8889, time 44.93ms\n",
      "step 960: train loss 1.6488, val loss 1.7613\n",
      "iter 960: loss 1.9252, time 542.85ms\n",
      "iter 970: loss 1.5865, time 44.78ms\n",
      "iter 980: loss 1.6497, time 45.61ms\n",
      "step 990: train loss 1.6464, val loss 1.7376\n",
      "iter 990: loss 1.9733, time 574.53ms\n",
      "iter 1000: loss 1.5010, time 44.84ms\n"
     ]
    }
   ],
   "source": [
    "#scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    logits, loss = model(X, Y)\n",
    "    X, Y = get_batch('train')\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #scaler.scale(loss).backward()\n",
    "    loss.backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        #scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    #scaler.step(optimizer)\n",
    "    #scaler.update()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        lossf = loss.item()\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e50290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.678484320640564"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87414534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
