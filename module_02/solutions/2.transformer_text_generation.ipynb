{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKYJvrZvtBS"
      },
      "source": [
        "# Transformers in Action\n",
        "\n",
        "We will now focus on the key components that make transformers so impactful and go through some hands-on exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOtZ_iTqx8aB"
      },
      "source": [
        "## Attention is All you Need ‚ö†Ô∏è\n",
        "We leveraged a basic RNN based network to generate text in the previous notebook. To enhance performance of sequence to sequence tasks a typical Encoder-Decoder architecture is the go-to choice.\n",
        "\n",
        "<img src=\"../../assets/module_2/encoder_decoder_nb_2.png\">\n",
        "\n",
        "\n",
        "Let us consider the case of **Machine Translation**, i.e. translation of English to Spanish (or any other language).\n",
        "\n",
        "In a typical __Encoder-Decoder__ architecture, the Encoder takes in the input text in English as input and prepares a condensed vector representation of the whole input. Typically termed as bottleneck features. The Decoder then uses these features to generate the translated text in Spanish.\n",
        "\n",
        "While this architecture and its variants worked wonders, they had issues. Issues such as inability handle longer input sequences, cases where there is not a one to one mapping between input vs output language and so on.\n",
        "\n",
        "To handle these issues, __Vasvani et. al.__ in their now famouly titled paper __Attention Is All You Need__ build up on the concepts of attention. The main highlight of this work was the Transformer architecture. Transformers were shown to present state of the art results on multiple benchmarks without using any recurrence or convolutional components.\n",
        "\n",
        "\n",
        "### Attention & Self-Attention\n",
        "The concept of __Attention__ is a simple yet important one. In layman terms, it helps the model focus on not just the current input but also determine specific pieces of information from the past. This helps in models which are able to handle long range dependencies along with scenarios where there is not a one to one mapping between inputs and outputs. The following is a sample illustration from the paper demonstrating the focus/attention of the model on the words when making is the input.\n",
        "\n",
        "<img src=\"../../assets/module_2/attention_nb_2.png\">\n",
        "\n",
        "> Source: [Vasvani et. al.](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "\n",
        "__Self-attention__ is a mechanism that allows the transformer model to weigh the importance of different positions (or \"tokens\") __within__ a sequence when encoding or decoding.\n",
        "\n",
        "__Multi-head attention__ extends the self-attention mechanism by performing multiple parallel self-attention operations, each focusing on different learned linear projections of the input. Multiple attention heads allow the model to capture different types of relationships and learn more fine-grained representations (eg: grammar, context, dependency, etc.)\n",
        "\n",
        "<img src=\"../../assets/module_2/multihead_attention_nb_2.png\">\n",
        "\n",
        "> Source: [Vasvani et. al.](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "\n",
        "### Positional Encoding\n",
        "Positional encoding is a technique used to incorporate the position of each token in the input sequence. It provides the model with information about the token's position without relying solely on the order of tokens.\n",
        "This additional aspect was required because transformers do not have the natural sequential setup of RNNs. In order to provide positional context, any encoding system should ideally have the following properties:\n",
        "\n",
        "- It should output a unique encoding for each time-step (word‚Äôs position in a sentence)\n",
        "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
        "- Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
        "- It must be deterministic.\n",
        "\n",
        "<img src=\"../../assets/module_2/positional_emb_nb_2.png\">\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQkJaPxR9jMN"
      },
      "source": [
        "## Hugging Face ü§ó\n",
        "> On a mission to solve NLP, one commit at a time.\n",
        "\n",
        "As their tagline explains, they are helping solve NLP problems. While the transformer revolution changed things for language related tasks, using them was not a simple thing. With number of parameters running into billions, these models were out of reach for most researchers and application developers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_02/solutions/2.transformer_text_generation.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSpcCmwm9iqp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE419FiepIUd"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaamicxVx3E2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from numpy import random\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MV8eiskZw0C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,\n",
        "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__gIyUZBZa4-"
      },
      "outputs": [],
      "source": [
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37P36nBNvqW3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG-jgv4xZd4l"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxRJDzAwZe-c"
      },
      "source": [
        "## Get Data\n",
        "We will fine-tune a pre-trained model GPT-2 model on our earlier dataset itself. But wait, what do you mean pre-trained?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVooYvMv9w4A"
      },
      "outputs": [],
      "source": [
        "!wget -O sherlock_homes.txt http://www.gutenberg.org/files/1661/1661-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR85DAZ3908r"
      },
      "outputs": [],
      "source": [
        "filename = \"sherlock_homes.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "text = raw_text [1450:100000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnNK4X8zZ3nC"
      },
      "source": [
        "## Foundation & Pre-trained Models\n",
        "\n",
        "**Foundation models** are the models that are trained from scratch on a large corpus of data. In the context of NLP, these models are designed to learn the fundamental patterns, structures, and representations of natural language. Foundation models are typically trained using unsupervised learning objectives, such as language modeling or autoencoding, where the model predicts the next word in a sentence or reconstructs the original sentence from a corrupted version/masked version.\n",
        "Models such as GPT, BERT, T5, etc are typical examples of Foundation Models\n",
        "\n",
        "\n",
        "Instances of foundation models that have been trained on specific downstream tasks or datasets are termed as **Pre-Trained Models**. Pretrained models leverage the knowledge learned from foundation models and are fine-tuned on task-specific data to perform well on specific NLP tasks, such as text classification, named entity recognition, machine translation, sentiment analysis, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tgmDmrInbH7"
      },
      "outputs": [],
      "source": [
        "BOS_TOKEN = '<|sot|>'\n",
        "EOS_TOKEN = '<|eot|>'\n",
        "PAD_TOKEN = '<|pad|>'\n",
        "MODEL_NAME = \"raghavbali/gpt2_ft_sherlock_holmes\"\n",
        "#'gpt2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkUd7xYF90qf"
      },
      "outputs": [],
      "source": [
        "# first, let us get the tokenizer object\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME,\n",
        "                                          bos_token=BOS_TOKEN,\n",
        "                                          eos_token=EOS_TOKEN,\n",
        "                                          pad_token=PAD_TOKEN\n",
        "                                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bg5DyUanUuH"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X-OQdqpnXMA"
      },
      "outputs": [],
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer(\n",
        "          BOS_TOKEN + txt + EOS_TOKEN,\n",
        "          truncation=True,\n",
        "          max_length=max_length,\n",
        "          padding=\"max_length\"\n",
        "          )\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khd_WdbL1Pg3"
      },
      "outputs": [],
      "source": [
        "# set batch size to work it out on colab\n",
        "BATCH_SIZE = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIEfRq8_n_Bx"
      },
      "outputs": [],
      "source": [
        "dataset = GPT2Dataset(text.split('\\n'),\n",
        "                      tokenizer, max_length=768)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmDV2ZEPoAvT"
      },
      "outputs": [],
      "source": [
        "# Create the DataLoaders for our training and validation datasets.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = BATCH_SIZE\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = BATCH_SIZE\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzyrweQsoV0N"
      },
      "source": [
        "## Setup Model Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbJeRwyzoxQB"
      },
      "outputs": [],
      "source": [
        "# Training Params\n",
        "epochs = 1 #3 seems good if you train from gpt2 checkpoint\n",
        "learning_rate = 5e-4\n",
        "# to speed up learning\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# generate output after N steps\n",
        "sample_every = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si9CxaiAoXYA"
      },
      "outputs": [],
      "source": [
        "# Set Config\n",
        "configuration = GPT2Config.from_pretrained(MODEL_NAME,\n",
        "                                           output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME, config=configuration,)\n",
        "\n",
        "# NOTE: This is important to imply that we have updated BOS, EOS, etc\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFQpA-AWoweA"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkLtB1Ako_qY"
      },
      "outputs": [],
      "source": [
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycp5eO6zpQaA"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiMynWmrpS1j"
      },
      "outputs": [],
      "source": [
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # Training\n",
        "    print(\"*\"*25)\n",
        "    print('>> Epoch {:} / {:} '.format(epoch_i + 1, epochs))\n",
        "    print(\"*\"*25)\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels,\n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Training Loss: {:>5,}.   Time Taken: {:}.'.format(step,\n",
        "                                                                                     len(train_dataloader),\n",
        "                                                                                     batch_loss,\n",
        "                                                                                     elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    do_sample=True,\n",
        "                                    top_k=50,\n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1,\n",
        "                                    pad_token_id=tokenizer.eos_token_id\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Average Loss\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # training time\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"Training epoch time: {:}\".format(training_time))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs  = model(b_input_ids,\n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation time: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_oss': avg_val_loss,\n",
        "            'train_ime': training_time,\n",
        "            'val_ime': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training Completed\")\n",
        "print(\"Total training time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRPYndqCq6rb"
      },
      "outputs": [],
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLUicMZOrfu-"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzsK7aQHrfOT"
      },
      "outputs": [],
      "source": [
        "output_dir = './model_save/'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6cYK9IerAYi"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"i am writing this prompt\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(BOS_TOKEN+prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                max_length = len(generated) + 50,\n",
        "                                top_p=0.92,\n",
        "                                num_return_sequences=3,\n",
        "                                pad_token_id=tokenizer.eos_token_id,\n",
        "                                temperature=0.8,\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htgSYctArNGu"
      },
      "outputs": [],
      "source": [
        "# compare output to foundation model\n",
        "pre_trainedtokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "pre_trainedmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPOixxDirXjC"
      },
      "outputs": [],
      "source": [
        "input_ids = pre_trainedtokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = pre_trainedmodel.generate(\n",
        "    input_ids,\n",
        "    bos_token_id=random.randint(1,30000),\n",
        "    max_length=len(input_ids[0]) + 50,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,  # Adjust the sampling parameters as needed\n",
        "    temperature=0.8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb1Ftef4rbQl"
      },
      "outputs": [],
      "source": [
        "pre_trainedtokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
